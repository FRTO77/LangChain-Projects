import os
from typing import Optional, List

import streamlit as st
from dotenv import load_dotenv


def load_env() -> None:
    """Load environment variables from a local .env file if present."""
    try:
        load_dotenv()
    except Exception:
        pass


def read_text_from_file(uploaded_file) -> str:
    """Read text from a Streamlit uploaded file (.txt or .pdf)."""
    if uploaded_file is None:
        return ""

    filename = uploaded_file.name.lower()
    if filename.endswith(".txt"):
        raw_bytes = uploaded_file.read()
        try:
            return raw_bytes.decode("utf-8")
        except Exception:
            return raw_bytes.decode("latin-1", errors="ignore")

    if filename.endswith(".pdf"):
        try:
            from pypdf import PdfReader
        except Exception as exc:
            st.error("–î–ª—è —á—Ç–µ–Ω–∏—è PDF —Ç—Ä–µ–±—É–µ—Ç—Å—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å 'pypdf'. –î–æ–±–∞–≤—å—Ç–µ –µ—ë –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–µ.")
            raise exc

        reader = PdfReader(uploaded_file)
        pages_text: List[str] = []
        for page in reader.pages:
            try:
                pages_text.append(page.extract_text() or "")
            except Exception:
                pages_text.append("")
        return "\n\n".join(pages_text)

    st.warning("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã .txt –∏ .pdf")
    return ""


def make_llm(provider: str, model: str, api_key: Optional[str], temperature: float = 0.2):
    """Create an LLM instance for the chosen provider."""
    if provider == "OpenAI":
        try:
            from langchain_openai import ChatOpenAI
        except Exception as exc:
            st.error("–ù–µ –Ω–∞–π–¥–µ–Ω–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ 'langchain-openai'. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏–∑ requirements.txt")
            raise exc

        effective_key = api_key or os.getenv("OPENAI_API_KEY")
        if not effective_key:
            st.stop()
        return ChatOpenAI(model=model, api_key=effective_key, temperature=temperature)

    if provider == "Ollama":
        try:
            from langchain_ollama import ChatOllama
        except Exception as exc:
            st.error("–ù–µ –Ω–∞–π–¥–µ–Ω–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ 'langchain-ollama'. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏–∑ requirements.txt")
            raise exc

        base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        return ChatOllama(model=model, base_url=base_url, temperature=temperature)

    raise ValueError(f"Unknown provider: {provider}")


def chunk_text(text: str, chunk_size: int = 2000, chunk_overlap: int = 200) -> List[str]:
    """Split long text into chunks using LangChain's RecursiveCharacterTextSplitter if available.
    Fallback to a simple splitter by characters.
    """
    text = (text or "").strip()
    if not text:
        return []

    try:
        from langchain.text_splitter import RecursiveCharacterTextSplitter

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ". ", ".", "? ", "! ", ", ", ",", " "]
        )
        docs = splitter.create_documents([text])
        return [d.page_content for d in docs]
    except Exception:
        # Simple fallback: naive split by characters with safe stepping
        chunks: List[str] = []
        n = len(text)
        safe_overlap = max(0, min(chunk_overlap, chunk_size - 1))
        step = max(1, chunk_size - safe_overlap)
        i = 0
        while i < n:
            end = min(i + chunk_size, n)
            chunks.append(text[i:end])
            if end >= n:
                break
            i += step
        return chunks


def build_chunk_prompt(
    chunk: str,
    target_length: str,
    bullet_points: bool,
    language_pref: str,
) -> str:
    """Prompt to summarize a single chunk."""
    formatting = (
        "–°—Ñ–æ—Ä–º–∏—Ä—É–π –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏–∑ 5-10 –ø—É–Ω–∫—Ç–æ–≤" if bullet_points else "–°—Ñ–æ—Ä–º–∏—Ä—É–π —Å–≤—è–∑–Ω—ã–π –∞–±–∑–∞—Ü –∏–∑ 5-8 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"
    )
    language_instruction = (
        "–û—Ç–≤–µ—Ç—å –Ω–∞ —Ç–æ–º –∂–µ —è–∑—ã–∫–µ, —á—Ç–æ –∏ –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç." if language_pref == "–ê–≤—Ç–æ" else f"–û—Ç–≤–µ—Ç—å –Ω–∞ {language_pref}."
    )

    return (
        f"–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–æ–Ω—Å–ø–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é. –°–æ–∂–º–∏ —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç –≤ {target_length} –∫–æ–Ω—Å–ø–µ–∫—Ç –¥–ª—è –∑–∞–Ω—è—Ç–æ–≥–æ —á–∏—Ç–∞—Ç–µ–ª—è.\n\n"
        f"–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:\n"
        f"- {formatting}\n"
        f"- {language_instruction}\n"
        f"- –°–æ—Ö—Ä–∞–Ω—è–π –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã, —Ü–∏—Ñ—Ä—ã, –∏–º–µ–Ω–∞, –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏\n"
        f"- –ò–∑–±–µ–≥–∞–π –≤–æ–¥—ã –∏ –ø–æ–≤—Ç–æ—Ä–æ–≤, –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤\n\n"
        f"–¢–µ–∫—Å—Ç:\n{chunk}\n"
    )


def build_combine_prompt(
    partial_summaries: str,
    target_length: str,
    bullet_points: bool,
    language_pref: str,
) -> str:
    formatting = (
        "–°—Ñ–æ—Ä–º–∏—Ä—É–π –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏–∑ 5-12 –ø—É–Ω–∫—Ç–æ–≤" if bullet_points else "–°—Ñ–æ—Ä–º–∏—Ä—É–π —Å–≤—è–∑–Ω—ã–π –∞–±–∑–∞—Ü(—ã) –∏–∑ 8-15 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"
    )
    language_instruction = (
        "–û—Ç–≤–µ—Ç—å –Ω–∞ —Ç–æ–º –∂–µ —è–∑—ã–∫–µ, —á—Ç–æ –∏ –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç." if language_pref == "–ê–≤—Ç–æ" else f"–û—Ç–≤–µ—Ç—å –Ω–∞ {language_pref}."
    )

    return (
        f"–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–∂–∞—Ç–∏—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –û–±—ä–µ–¥–∏–Ω–∏ —á–∞—Å—Ç–∏—á–Ω—ã–µ –∫–æ–Ω—Å–ø–µ–∫—Ç—ã –Ω–∏–∂–µ –≤ –æ–¥–∏–Ω —Ü–µ–ª—å–Ω—ã–π {target_length} –∫–æ–Ω—Å–ø–µ–∫—Ç.\n\n"
        f"–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:\n"
        f"- {formatting}\n"
        f"- {language_instruction}\n"
        f"- –°–æ—Ö—Ä–∞–Ω—è–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–∞\n\n"
        f"–ß–∞—Å—Ç–∏—á–Ω—ã–µ –∫–æ–Ω—Å–ø–µ–∫—Ç—ã:\n{partial_summaries}\n"
    )


def call_llm(llm, prompt: str) -> str:
    """Call the chat model with a system+user style prompt packed into a single user message."""
    try:
        # Many LangChain chat models accept plain strings via .invoke
        result = llm.invoke(prompt)
        # For Chat models, content is on .content
        content = getattr(result, "content", None)
        return content if isinstance(content, str) and content.strip() else (str(result) if result else "")
    except Exception as exc:
        st.error(f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ LLM: {exc}")
        raise


def summarize_long_text(
    llm,
    text: str,
    target_length: str,
    bullet_points: bool,
    language_pref: str,
) -> str:
    """Chunk the text, summarize each chunk, then combine."""
    chunks = chunk_text(text)
    if not chunks:
        return ""

    if len(chunks) == 1:
        single_prompt = build_chunk_prompt(chunks[0], target_length, bullet_points, language_pref)
        return call_llm(llm, single_prompt)

    partials: List[str] = []
    for idx, ch in enumerate(chunks, start=1):
        with st.spinner(f"–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ {idx}/{len(chunks)}‚Ä¶"):
            partials.append(call_llm(llm, build_chunk_prompt(ch, target_length, bullet_points, language_pref)))

    combined_prompt = build_combine_prompt("\n\n".join(partials), target_length, bullet_points, language_pref)
    return call_llm(llm, combined_prompt)


def main():
    load_env()

    st.set_page_config(page_title="AI‚Äë–ö–æ–Ω—Å–ø–µ–∫—Ç–æ—Ä", page_icon="üìù", layout="centered")
    st.title("üìù AI‚Äë–∫–æ–Ω—Å–ø–µ–∫—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞")
    st.caption("Python + LangChain + OpenAI/Ollama + Streamlit")

    with st.sidebar:
        st.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏")
        provider = st.selectbox("–ü—Ä–æ–≤–∞–π–¥–µ—Ä", ["OpenAI", "Ollama"], index=0)

        if provider == "OpenAI":
            default_model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
            model = st.selectbox("–ú–æ–¥–µ–ª—å (OpenAI)", ["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"], index=0)
            api_key = st.text_input("OPENAI_API_KEY", value=os.getenv("OPENAI_API_KEY", ""), type="password")
        else:
            default_model = os.getenv("OLLAMA_MODEL", "llama2")
            model = st.text_input("–ú–æ–¥–µ–ª—å (Ollama)", value=default_model, help="–ù–∞–ø—Ä–∏–º–µ—Ä: llama2, llama3, mistral")
            api_key = None

        target_length = st.radio(
            "–î–ª–∏–Ω–∞ –∫–æ–Ω—Å–ø–µ–∫—Ç–∞",
            options=["–ö–æ—Ä–æ—Ç–∫–∏–π", "–°—Ä–µ–¥–Ω–∏–π", "–î–ª–∏–Ω–Ω—ã–π"],
            index=1,
            help="–ö–æ—Ä–æ—Ç–∫–∏–π ‚âà 3‚Äì5 –ø—É–Ω–∫—Ç–æ–≤, –°—Ä–µ–¥–Ω–∏–π ‚âà 6‚Äì10, –î–ª–∏–Ω–Ω—ã–π ‚âà 10‚Äì15"
        )
        bullet_points = st.toggle("–ú–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—É–Ω–∫—Ç—ã", value=True)
        language_pref = st.selectbox("–Ø–∑—ã–∫ –≤—ã–≤–æ–¥–∞", ["–ê–≤—Ç–æ", "–†—É—Å—Å–∫–∏–π", "English"], index=0)

    st.subheader("–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
    tab_text, tab_file = st.tabs(["–í—Å—Ç–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç", "–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª (.txt/.pdf)"])

    with tab_text:
        input_text = st.text_area(
            "–¢–µ–∫—Å—Ç –¥–ª—è –∫–æ–Ω—Å–ø–µ–∫—Ç–∞",
            height=240,
            placeholder="–í—Å—Ç–∞–≤—å—Ç–µ –∏–ª–∏ –Ω–∞–ø–∏—à–∏—Ç–µ —Å—é–¥–∞ –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç‚Ä¶",
        ).strip()

    with tab_file:
        uploaded = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª", type=["txt", "pdf"], accept_multiple_files=False)
        if uploaded is not None and not input_text:
            input_text = read_text_from_file(uploaded)

    if st.button("–°–∂–∞—Ç—å —Ç–µ–∫—Å—Ç"):
        if not input_text:
            st.warning("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª.")
            st.stop()

        with st.spinner("–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å‚Ä¶"):
            llm = make_llm(provider=provider, model=model, api_key=api_key, temperature=0.2)

        with st.spinner("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–Ω—Å–ø–µ–∫—Ç‚Ä¶"):
            summary = summarize_long_text(
                llm=llm,
                text=input_text,
                target_length=target_length,
                bullet_points=bullet_points,
                language_pref=language_pref,
            )

        if summary:
            st.success("–ì–æ—Ç–æ–≤–æ!")
            st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç")
            st.write(summary)
            st.download_button("‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å –∫–∞–∫ TXT", data=summary, file_name="summary.txt")
        else:
            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Å–ø–µ–∫—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑ –∏–ª–∏ –∏–∑–º–µ–Ω–∏—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.")


if __name__ == "__main__":
    main()


